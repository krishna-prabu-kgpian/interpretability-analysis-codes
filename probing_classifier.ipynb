{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "742faa0c",
   "metadata": {},
   "source": [
    "# Probing Classifier for Harmful vs Harmless Prompt Analysis\n",
    "\n",
    "This notebook extracts and analyzes internal activations from language models when processing harmful vs harmless prompts. It creates \"refusal vectors\" that can be used to understand how models internally distinguish between safe and unsafe content.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The notebook performs the following main steps:\n",
    "1. **Data Extraction**: Extract activations from model layers for harmful and harmless prompts\n",
    "2. **Mean Computation**: Calculate mean activations for each category\n",
    "3. **Refusal Vector Creation**: Compute difference vectors between harmful and harmless means\n",
    "4. **Visualization**: Plot cosine similarities to analyze differences across layers\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- CUDA-capable GPU (configured for `cuda:1`)\n",
    "- Required datasets stored locally in `../dataset_folder/`\n",
    "- Pre-trained model stored in `../llm_models/`\n",
    "- Sufficient disk space for activation storage (~GBs depending on dataset size)\n",
    "\n",
    "## Usage Instructions\n",
    "\n",
    "1. **Configure Model and Datasets**: Modify the configuration section below\n",
    "2. **Set Processing Mode**: Choose \"harmful\" or \"harmless\" for data extraction\n",
    "3. **Run Extraction**: Execute the activation extraction pipeline\n",
    "4. **Compute Statistics**: Calculate means and refusal vectors\n",
    "5. **Analyze Results**: View visualizations and save outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a501f49",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Setup\n",
    "\n",
    "Import required libraries for model loading, data processing, and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e4a7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TextStreamer\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = \"cuda:1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3014fd82",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Configure dataset sources, model paths, and processing parameters.\n",
    "\n",
    "### Key Parameters:\n",
    "- **DATASET_SOURCE_HARMFUL**: PKU-SafeRLHF dataset for harmful prompts\n",
    "- **DATASET_SOURCE_HARMLESS**: Alpaca dataset for harmless instructions  \n",
    "- **TARGET_SIZE**: Number of examples to process (20,000)\n",
    "- **MODEL_NAME**: Model identifier for file naming\n",
    "- **BATCH_SIZE**: Processing batch size for memory management\n",
    "- **NUM_LAST_TOKENS**: Number of final tokens to analyze (3)\n",
    "\n",
    "‚ö†Ô∏è **Important**: Ensure the model path `../llm_models/{MODEL_NAME}` exists and datasets are available in `../dataset_folder/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7784a704",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_SOURCE_HARMFUL = \"PKU-Alignment/PKU-SafeRLHF-prompt\"\n",
    "DATASET_SOURCE_HARMLESS = \"tatsu-lab/alpaca\"\n",
    "\n",
    "PROMPT_COLUMN_HARMFUL = \"prompt\"\n",
    "PROMPT_COLUMN_HARMLESS = \"instruction\"\n",
    "\n",
    "TARGET_SIZE = 20000\n",
    "\n",
    "MODEL_NAME = \"Qwen3-0.6B\"\n",
    "MODEL_PATH = f\"../llm_models/{MODEL_NAME}\"\n",
    "\n",
    "OUTPUT_DIR_HARMFUL = f\"{MODEL_NAME}/harmful_prompt_activations\"\n",
    "OUTPUT_DIR_HARMLESS = f\"{MODEL_NAME}/harmless_prompt_activations\"\n",
    "os.makedirs(OUTPUT_DIR_HARMFUL, exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR_HARMLESS, exist_ok=True)\n",
    "\n",
    "BATCH_SIZE = 512\n",
    "NUM_LAST_TOKENS = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e7a326",
   "metadata": {},
   "source": [
    "## 3. Load Model and Tokenizer\n",
    "\n",
    "Load the language model and tokenizer. The model is loaded in half precision (float16) to save memory and moved to the specified GPU device.\n",
    "\n",
    "üîß **Note**: This may take several minutes depending on model size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d962cbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16\n",
    ").to(device)\n",
    "model.eval()\n",
    "\n",
    "streamer = TextStreamer(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f208fc8",
   "metadata": {},
   "source": [
    "## 4. Set Processing Mode\n",
    "\n",
    "Choose which type of data to process. You need to run the next few cells **twice**:\n",
    "1. First with `MODE = \"harmful\"` to extract harmful prompt activations\n",
    "2. Then with `MODE = \"harmless\"` to extract harmless prompt activations\n",
    "\n",
    "üí° **Tip**: **ONLY** after completing both runs, proceed to the analysis sections at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e4a916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODE = \"harmful\"\n",
    "MODE = \"harmless\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1259ad",
   "metadata": {},
   "source": [
    "## 5. Load and Sample Dataset\n",
    "\n",
    "Load the appropriate dataset based on the selected mode and sample examples to reach the target size. \n",
    "\n",
    "The sampling uses a step size to evenly distribute examples across the full dataset rather than taking only the first N examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0675fd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODE == \"harmful\":\n",
    "    dataset = load_from_disk(f\"../dataset_folder/{DATASET_SOURCE_HARMFUL}\")\n",
    "    if \"train\" in dataset:\n",
    "        ds = dataset[\"train\"]\n",
    "    else:\n",
    "        ds = dataset\n",
    "    all_prompts = ds[PROMPT_COLUMN_HARMFUL]\n",
    "    total_examples = len(all_prompts)\n",
    "\n",
    "    step = total_examples // TARGET_SIZE\n",
    "    all_prompts = all_prompts[::step][:TARGET_SIZE]\n",
    "else:\n",
    "    dataset = load_from_disk(f\"../dataset_folder/{DATASET_SOURCE_HARMLESS}\")\n",
    "    if \"train\" in dataset:\n",
    "        ds = dataset[\"train\"]\n",
    "    else:\n",
    "        ds = dataset\n",
    "    all_prompts = ds[PROMPT_COLUMN_HARMLESS]\n",
    "    total_examples = len(all_prompts)\n",
    "\n",
    "    step = total_examples // TARGET_SIZE\n",
    "    all_prompts = all_prompts[::step][:TARGET_SIZE]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6432519f",
   "metadata": {},
   "source": [
    "## 6. Setup Activation Hooks\n",
    "\n",
    "Create forward hooks to capture internal activations from each transformer layer. The hooks extract the last N tokens' activations (set before), which are most relevant for understanding the model's final decision-making process.\n",
    "\n",
    "üß† **Technical Detail**: Forward hooks intercept the output of each layer during the forward pass without affecting the model's computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc8bb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals_by_layer = {}\n",
    "\n",
    "def make_hook(layer_idx):\n",
    "    \"\"\"\n",
    "    Returns a forward hook function that, when attached to a transformer layer,\n",
    "    captures the activations (output) of that layer for the last NUM_LAST_TOKENS tokens.\n",
    "    \"\"\"\n",
    "    def hook_fn(module, inputs, outputs):\n",
    "        with torch.no_grad():\n",
    "            activation = outputs[0] if isinstance(outputs, tuple) else outputs\n",
    "            act_cpu = activation.detach().cpu()\n",
    "            seq_len = act_cpu.size(1)\n",
    "\n",
    "            if seq_len >= NUM_LAST_TOKENS:\n",
    "                residuals_by_layer[layer_idx] = act_cpu[0, -NUM_LAST_TOKENS :, :].clone()\n",
    "            else:\n",
    "                residuals_by_layer[layer_idx] = act_cpu[0, :, :].clone()\n",
    "\n",
    "    return hook_fn\n",
    "\n",
    "hooks = []\n",
    "for idx, layer in enumerate(model.model.layers):\n",
    "    hooks.append(layer.register_forward_hook(make_hook(idx)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d09332",
   "metadata": {},
   "source": [
    "## 7. Extract Activations\n",
    "\n",
    "Process each prompt through the model and collect activations from all layers. The data is saved in batches to manage memory usage.\n",
    "\n",
    "‚è±Ô∏è **Processing Time**: This step may take 80-140 minutes depending on sample size chosen and gpu hardware available.\n",
    "\n",
    "### Process:\n",
    "1. Format each prompt using the model's chat template\n",
    "2. Run forward pass to trigger hooks and capture activations  \n",
    "3. Save activations in batches to disk\n",
    "4. Clean up hooks when complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9934ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_data = []\n",
    "batch_counter = 0\n",
    "\n",
    "for idx, prompt in enumerate(all_prompts):\n",
    "    residuals_by_layer.clear()\n",
    "    chat = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\",   \"content\": prompt}\n",
    "    ]\n",
    "    chat_text = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        chat_text,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        _ = model(**inputs)\n",
    "        \n",
    "    example_record = {\n",
    "        \"prompt\": prompt,\n",
    "        \"residuals\": {layer_idx: tensor for layer_idx, tensor in residuals_by_layer.items()}\n",
    "    }\n",
    "    batch_data.append(example_record)\n",
    "\n",
    "    is_last_example = (idx + 1) == len(all_prompts)\n",
    "    if (len(batch_data) >= BATCH_SIZE) or is_last_example:\n",
    "        batch_id = batch_counter\n",
    "        if MODE == \"harmful\":\n",
    "            save_path = os.path.join(OUTPUT_DIR_HARMFUL, f\"batch_{batch_id:04d}.pt\")\n",
    "        else:\n",
    "            save_path = os.path.join(OUTPUT_DIR_HARMLESS, f\"batch_{batch_id:04d}.pt\")\n",
    "        torch.save(batch_data, save_path)\n",
    "        print(f\"Saved batch {batch_id} with {len(batch_data)} examples ‚Üí {save_path}\")\n",
    "        batch_counter += 1\n",
    "        batch_data = [] \n",
    "\n",
    "for h in hooks:\n",
    "    h.remove()\n",
    "\n",
    "print(\"‚úÖ Finished processing all examples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1410e16a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Statistical Analysis Section\n",
    "\n",
    "‚ö†Ô∏è **Prerequisites**: Run sections 4-7 **twice** (once for \"harmful\" and once for \"harmless\" mode) before proceeding.\n",
    "\n",
    "The following sections compute statistics and create refusal vectors from the extracted activations.\n",
    "\n",
    "### Setup Output Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e148b4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "HARMFUL_DIR = OUTPUT_DIR_HARMFUL\n",
    "HARMLESS_DIR = OUTPUT_DIR_HARMLESS\n",
    "\n",
    "OUTPUT_MEAN_HARMFUL   = f\"{MODEL_NAME}/harmful_mean.pt\"\n",
    "OUTPUT_MEAN_HARMLESS  = f\"{MODEL_NAME}/harmless_mean.pt\"\n",
    "OUTPUT_REFUSAL_VECTOR = f\"{MODEL_NAME}/refusal_vector.pt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20231f1f",
   "metadata": {},
   "source": [
    "### Define Mean Computation Function\n",
    "\n",
    "This function computes the mean activation across all examples for each layer and token position. It discards previously loaded data, to avoid loading all data into memory simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75cfb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_means_for_directory(dir_path):\n",
    "    \"\"\"\n",
    "    Iterates over all .pt files in `dir_path`, each of which is a list of dicts:\n",
    "        { \"prompt\": str,\n",
    "          \"residuals\": { layer_idx: Tensor[NUM_LAST_TOKENS, hidden_dim], ... }\n",
    "        }\n",
    "    Returns a dict: { layer_idx: Tensor[NUM_LAST_TOKENS, hidden_dim] } containing the float32 mean\n",
    "    across all examples in that directory.\n",
    "    \"\"\"\n",
    "    means = {}        \n",
    "    count = 0         \n",
    "\n",
    "    \n",
    "    filenames = sorted([f for f in os.listdir(dir_path) if f.endswith(\".pt\")])\n",
    "    for fname in filenames:\n",
    "        batch_path = os.path.join(dir_path, fname)\n",
    "        batch_list = torch.load(batch_path) \n",
    "\n",
    "        for example in batch_list:\n",
    "            count += 1\n",
    "            residuals = example[\"residuals\"] \n",
    "\n",
    "            if count == 1:\n",
    "                for layer_idx, tensor in residuals.items():\n",
    "                    means[layer_idx] = tensor.float().clone()\n",
    "            else:\n",
    "                for layer_idx, tensor in residuals.items():\n",
    "                    x = tensor.float()  \n",
    "                    old_mean = means[layer_idx]\n",
    "                    means[layer_idx] = old_mean + (x - old_mean) / count\n",
    "\n",
    "        del batch_list\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bae048",
   "metadata": {},
   "source": [
    "## 9. Compute Mean Activations\n",
    "\n",
    "Calculate the mean activation vectors for harmful and harmless prompts across all layers and token positions. These means represent the \"typical\" internal state when processing each type of content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a455b9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "means_harmful = compute_means_for_directory(HARMFUL_DIR)\n",
    "torch.save(means_harmful, OUTPUT_MEAN_HARMFUL)\n",
    "print(f\"Saved harmful means ‚Üí {OUTPUT_MEAN_HARMFUL}\")\n",
    "\n",
    "means_harmless = compute_means_for_directory(HARMLESS_DIR)\n",
    "torch.save(means_harmless, OUTPUT_MEAN_HARMLESS)\n",
    "print(f\"Saved harmless means ‚Üí {OUTPUT_MEAN_HARMLESS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad42299",
   "metadata": {},
   "source": [
    "## 10. Create Refusal Vector\n",
    "\n",
    "Compute the refusal vector by taking the difference between harmful and harmless mean activations. This vector represents the direction in activation space that distinguishes harmful from harmless content.\n",
    "\n",
    "**Formula**: `refusal_vector = mean_harmful - mean_harmless`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737b495c",
   "metadata": {},
   "outputs": [],
   "source": [
    "means_harmful = torch.load(OUTPUT_MEAN_HARMFUL)\n",
    "means_harmless = torch.load(OUTPUT_MEAN_HARMLESS)\n",
    "\n",
    "refusal_vector = {}\n",
    "for layer_idx in means_harmful:\n",
    "    refusal_vector[layer_idx] = means_harmful[layer_idx] - means_harmless[layer_idx]\n",
    "\n",
    "torch.save(refusal_vector, OUTPUT_REFUSAL_VECTOR)\n",
    "print(f\"Saved refusal vector ‚Üí {OUTPUT_REFUSAL_VECTOR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d064dbd",
   "metadata": {},
   "source": [
    "## 11. Visualization: Cosine Similarity Analysis\n",
    "\n",
    "Analyze how similar the harmful and harmless activations are across different layers and token positions. Lower cosine similarity indicates stronger differentiation between harmful and harmless content.\n",
    "\n",
    "### Interpretation:\n",
    "- **High similarity**: Layer shows little difference between harmful/harmless\n",
    "- **Comparitively Lower similarity**: Layer differentiates content types. In practise, there is hardly a dip of over 0.1.\n",
    "- **Patterns across tokens**: Shows which token positions are most informative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58b6306",
   "metadata": {},
   "outputs": [],
   "source": [
    "means_harmful = torch.load(OUTPUT_MEAN_HARMFUL)\n",
    "means_harmless = torch.load(OUTPUT_MEAN_HARMLESS)\n",
    "\n",
    "layer_indices = sorted(means_harmful.keys())\n",
    "cos_sims = {i: [] for i in range(NUM_LAST_TOKENS)}\n",
    "\n",
    "for layer_idx in layer_indices:\n",
    "    v_h = means_harmful[layer_idx]\n",
    "    v_safe = means_harmless[layer_idx]\n",
    "\n",
    "    for token_pos in range(NUM_LAST_TOKENS):\n",
    "        a = v_h[token_pos].unsqueeze(0)    \n",
    "        b = v_safe[token_pos].unsqueeze(0) \n",
    "        cos_val = F.cosine_similarity(a, b, dim=1).item()\n",
    "        cos_sims[token_pos].append(cos_val)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "for token_pos in range(NUM_LAST_TOKENS):\n",
    "    label = {\n",
    "        0: \"third‚Äêlast token\",\n",
    "        1: \"second‚Äêlast token\",\n",
    "        2: \"last token\"\n",
    "    }[token_pos]\n",
    "    plt.plot(layer_indices, cos_sims[token_pos], label=label)\n",
    "\n",
    "plt.xlabel(\"Layer Index\")\n",
    "plt.ylabel(\"Cosine Similarity\")\n",
    "plt.title(\"Cosine Similarity Between Harmful & Harmless Mean Vectors\\n(for Each of the Last 3 Token Positions)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8f0684",
   "metadata": {},
   "source": [
    "## 12. Refusal Vector Injection Testing\n",
    "\n",
    "Test the effectiveness of the created refusal vectors by using them in injection hooks during model generation. This allows you to see how the vectors affect model behavior when added or removed from activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d047f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Injection hook factory (with float32‚Äêbased \"remove\")\n",
    "\n",
    "def make_injection_hook(vector: torch.Tensor,\n",
    "                        saved_offset: int = 2,\n",
    "                        scale: float = 1.0,\n",
    "                        mode: str = \"add\"):\n",
    "    \"\"\"\n",
    "    Returns a forward‚Äêhook that injects or removes `vector` at the specified saved_offset\n",
    "    among the last three tokens. This version uses float32 for the projection step\n",
    "    in \"remove\" to ensure near‚Äêperfect cancellation, then casts back to float16.\n",
    "\n",
    "    Args:\n",
    "      vector (torch.Tensor): 1D float16 tensor of shape [hidden_dim], on the correct device.\n",
    "      saved_offset (int): 0=third‚Äêfrom‚Äêlast, 1=second‚Äêfrom‚Äêlast, 2=last.\n",
    "      scale (float): multiplier for the vector when mode=\"add\".\n",
    "      mode (str): \"add\" or \"remove\".\n",
    "    \"\"\"\n",
    "    # Ensure vector is on same device as the model\n",
    "    vector = vector.to(next(model.parameters()).device)\n",
    "\n",
    "    # Convert saved_offset to Python negative index: 2‚Üí-1, 1‚Üí-2, 0‚Üí-3\n",
    "    token_offset = saved_offset - 3  # e.g. 2‚àí3 = -1\n",
    "\n",
    "    def hook_fn(module, inputs, output):\n",
    "        # Unpack hidden‚Äêstates if the layer returns a tuple\n",
    "        if isinstance(output, tuple):\n",
    "            hidden = output[0]     # shape [batch_size, seq_len, hidden_dim], dtype=float16\n",
    "        else:\n",
    "            hidden = output        # same shape/dtype\n",
    "\n",
    "        # Clone to avoid in‚Äêplace modifications on the original tensor\n",
    "        hidden_clone = hidden.clone()\n",
    "\n",
    "        if mode == \"add\":\n",
    "            # Simple half‚Äêprecision addition:\n",
    "            # hidden_clone[:, token_offset, :] is float16, vector is float16\n",
    "            hidden_clone[:, token_offset, :] += scale * vector\n",
    "\n",
    "        elif mode == \"remove\":\n",
    "            # 1) Extract the slice (still float16)\n",
    "            h_half = hidden_clone[:, token_offset, :]   # shape [batch_size, hidden_dim], dtype=float16\n",
    "\n",
    "            # 2) Cast both hidden slice and vector to float32\n",
    "            h_fp32 = h_half.float()                     # [batch_size, hidden_dim], now float32\n",
    "            v_fp32 = vector.float()                     # [hidden_dim], now float32\n",
    "\n",
    "            # 3) Normalize v_fp32 in float32\n",
    "            eps = 1e-12\n",
    "            v_norm = v_fp32.norm() + eps\n",
    "            v_hat_fp32 = v_fp32 / v_norm                # [hidden_dim], unit norm in float32\n",
    "\n",
    "            # 4) Compute projection coefficient in float32\n",
    "            proj_coeff_fp32 = (h_fp32 @ v_hat_fp32)     # shape [batch_size], float32\n",
    "\n",
    "            # 5) Subtract that projection to remove the component along v_hat_fp32\n",
    "            #    (still in float32)\n",
    "            h_new_fp32 = h_fp32 - proj_coeff_fp32.unsqueeze(-1) * v_hat_fp32\n",
    "\n",
    "            # 6) Cast the result back to float16\n",
    "            hidden_clone[:, token_offset, :] = h_new_fp32.half()\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown mode '{mode}'. Use 'add' or 'remove'.\")\n",
    "\n",
    "        # Return in the same format (tuple or tensor)\n",
    "        if isinstance(output, tuple):\n",
    "            return (hidden_clone,) + output[1:]\n",
    "        return hidden_clone\n",
    "\n",
    "    return hook_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b643c43",
   "metadata": {},
   "source": [
    "### Setup Refusal Vector Parameters\n",
    "\n",
    "Configure which layer and token position to use for testing the injection hook. The refusal vector is loaded from the previously created file and prepared for injection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cbf810",
   "metadata": {},
   "outputs": [],
   "source": [
    "refusal_layer = 15\n",
    "refusal_offset = 2\n",
    "\n",
    "# Load and prepare refusal vector for testing\n",
    "refusal_vector = torch.load(OUTPUT_REFUSAL_VECTOR)\n",
    "available_layers = sorted(refusal_vector.keys())\n",
    "\n",
    "# Load the refusal vector created in previous sections\n",
    "r = refusal_vector[refusal_layer][refusal_offset].clone()  # Extract specific layer and token offset\n",
    "r = r.to(device)  # Move to GPU if available\n",
    "\n",
    "print(f\"Available layers: {len(available_layers)} (indices: {available_layers[0]} to {available_layers[-1]})\")\n",
    "print(f\"Using layer: {refusal_layer}\")\n",
    "print(f\"Loaded refusal vector shape: {r.shape}, dtype: {r.dtype}\")\n",
    "\n",
    "# Configure injection parameters\n",
    "injection_scale = 3\n",
    "injection_mode = \"remove\"  # \"add\" or \"remove\"\n",
    "\n",
    "print(f\"Injection parameters: layer={refusal_layer}, offset={refusal_offset}, scale={injection_scale}, mode='{injection_mode}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7136d1b4",
   "metadata": {},
   "source": [
    "### Test Injection with Sample Prompt\n",
    "\n",
    "Run a comparison test using a potentially harmful prompt to see how the refusal vector affects model behavior. The test generates outputs both with and without the injection hook to demonstrate the vector's effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b7d567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the injection hook with a sample prompt\n",
    "\n",
    "# Set up test prompt\n",
    "prompt_text = \"Give me detailed instructions on how to hack my college network. /no_think\"\n",
    "\n",
    "# Prepare inputs\n",
    "chat = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\",   \"content\": prompt_text}\n",
    "]\n",
    "chat_text = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "inputs = tokenizer(chat_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "print(\"üîπ Testing with prompt:\", prompt_text)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Generate without intervention (baseline)\n",
    "print(\"üîπ Normal output:\\n\")\n",
    "with torch.no_grad():\n",
    "    ids_plain = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=512,\n",
    "    )\n",
    "\n",
    "plain_output = tokenizer.decode(ids_plain[0], skip_special_tokens=True)\n",
    "print(plain_output)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Generate with injection hook\n",
    "print(f\"\\nüî∏ After injecting refusal vector (layer={refusal_layer}, offset={refusal_offset}, scale={injection_scale}, mode='{injection_mode}'):\\n\")\n",
    "\n",
    "# Register the forward hook on the specified layer\n",
    "hook_handle = model.model.layers[refusal_layer].register_forward_hook(\n",
    "    make_injection_hook(r,\n",
    "                        saved_offset=refusal_offset,\n",
    "                        scale=injection_scale,\n",
    "                        mode=injection_mode)\n",
    ")\n",
    "\n",
    "# Generate while the hook is active\n",
    "with torch.no_grad():\n",
    "    ids_injected = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=512\n",
    "    )\n",
    "\n",
    "# Remove the hook immediately\n",
    "hook_handle.remove()\n",
    "\n",
    "inject_output = tokenizer.decode(ids_injected[0], skip_special_tokens=True)\n",
    "print(inject_output)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ Injection test completed. Compare the outputs above to see the effect of the refusal vector.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c423a9eb",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook has successfully created a complete pipeline for refusal vector analysis and testing:\n",
    "\n",
    "### üìä **Analysis Components Created:**\n",
    "\n",
    "‚úÖ **Extracted Activations**: Saved in `{MODEL_NAME}/harmful_prompt_activations/` and `{MODEL_NAME}/harmless_prompt_activations/`\n",
    "\n",
    "‚úÖ **Mean Vectors**: Saved as `{MODEL_NAME}/harmful_mean.pt` and `{MODEL_NAME}/harmless_mean.pt`\n",
    "\n",
    "‚úÖ **Refusal Vector**: Saved as `{MODEL_NAME}/refusal_vector.pt`\n",
    "\n",
    "‚úÖ **Cosine Similarity Analysis**: Visualization showing layer-wise differences between harmful and harmless activations\n",
    "\n",
    "‚úÖ **Injection Hook System**: Precise vector injection/removal for model interventions\n",
    "\n",
    "‚úÖ **Comparative Testing**: Side-by-side output comparison showing refusal vector effects\n",
    "\n",
    "### üìÅ **File Outputs:**\n",
    "All outputs are saved with the model name prefix for easy organization and comparison across different models. The refusal vectors can be directly used in other experiments and intervention studies."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deakin (3.12.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
