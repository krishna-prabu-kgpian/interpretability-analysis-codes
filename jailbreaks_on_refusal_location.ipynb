{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347305f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== refusal_cosine_utils.py (or keep in a notebook cell) =====\n",
    "import math\n",
    "from typing import Dict, Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "NUM_LAST_TOKENS = 3\n",
    "SYSTEM_PROMPT = \"You are a helpful assistant.\"  # fixed system prompt\n",
    "\n",
    "\n",
    "# --------------------------- Chat templating ---------------------------\n",
    "\n",
    "def build_inputs_from_chat(tokenizer, user_prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Uses apply_chat_template with a fixed system prompt and the provided user prompt.\n",
    "    Returns the rendered chat text (string). Caller will tokenize & move to device.\n",
    "    \"\"\"\n",
    "    chat = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\",   \"content\": user_prompt},\n",
    "    ]\n",
    "    chat_text = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "    return chat_text\n",
    "\n",
    "\n",
    "# --------------------------- Hooking & Capture -------------------------\n",
    "\n",
    "def _make_layer_hook(residuals_by_layer: Dict[int, torch.Tensor], layer_idx: int, k: int):\n",
    "    def hook_fn(module, inputs, outputs):\n",
    "        with torch.no_grad():\n",
    "            activation = outputs[0] if isinstance(outputs, tuple) else outputs  # [bs, seq, d]\n",
    "            acts = activation.detach().float().cpu()\n",
    "            seq_len = acts.size(1)\n",
    "            t = min(seq_len, k)\n",
    "            residuals_by_layer[layer_idx] = acts[0, -t:, :].clone()  # [t, d]\n",
    "    return hook_fn\n",
    "\n",
    "\n",
    "def register_capture_hooks(model, k: int = NUM_LAST_TOKENS):\n",
    "    \"\"\"\n",
    "    Registers forward hooks on each transformer layer to capture the last-k token activations.\n",
    "    Returns (hooks, residuals_by_layer_dict).\n",
    "    \"\"\"\n",
    "    residuals_by_layer: Dict[int, torch.Tensor] = {}\n",
    "    hooks = []\n",
    "    for idx, layer in enumerate(model.model.layers):\n",
    "        hooks.append(layer.register_forward_hook(_make_layer_hook(residuals_by_layer, idx, k)))\n",
    "    return hooks, residuals_by_layer\n",
    "\n",
    "\n",
    "def run_and_capture(model, tokenizer, chat_text: str, device: str = \"cuda:0\"):\n",
    "    \"\"\"\n",
    "    Runs a no-grad forward pass on the given chat_text. Hooks must be registered beforehand.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(chat_text, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        _ = model(**inputs)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "# --------------------------- Cosine Similarity -------------------------\n",
    "\n",
    "def compute_cosine_table(\n",
    "    residuals_by_layer: Dict[int, torch.Tensor],\n",
    "    refusal_vector: Dict[int, torch.Tensor],\n",
    "    k: int = NUM_LAST_TOKENS\n",
    ") -> Tuple[np.ndarray, List[int]]:\n",
    "    \"\"\"\n",
    "    Compute cosine similarity per layer Ã— last-k positions.\n",
    "\n",
    "    residuals_by_layer[layer] -> [T, d] (T <= k)\n",
    "    refusal_vector[layer]     -> [k, d]  (your saved per-position vectors)\n",
    "\n",
    "    Returns:\n",
    "      sims_table: [num_layers, k] array (NaN where a position was unavailable)\n",
    "      layer_order: list of layer indices aligned to sims_table rows\n",
    "    \"\"\"\n",
    "    layer_order = sorted(set(residuals_by_layer.keys()).intersection(refusal_vector.keys()))\n",
    "    if not layer_order:\n",
    "        raise ValueError(\"No overlapping layers between captured activations and refusal vectors.\")\n",
    "\n",
    "    sims = np.full((len(layer_order), k), np.nan, dtype=np.float32)\n",
    "\n",
    "    for r, layer_idx in enumerate(layer_order):\n",
    "        R = residuals_by_layer[layer_idx].float()   # [T, d]\n",
    "        V = refusal_vector[layer_idx].float()       # [k, d]\n",
    "        T = R.size(0)\n",
    "        # Compare trailing positions: R[-T:] with V[-T:]\n",
    "        per_tok = F.cosine_similarity(R, V[-T:, :], dim=-1).cpu().numpy()  # [T]\n",
    "        sims[r, k - T : k] = per_tok\n",
    "\n",
    "    return sims, layer_order\n",
    "\n",
    "\n",
    "# --------------------------- Single Overlay Plot -----------------------\n",
    "\n",
    "def plot_tokens_overlay(\n",
    "    sims_table: np.ndarray,\n",
    "    layer_order: List[int],\n",
    "    k: int = NUM_LAST_TOKENS,\n",
    "    title: str = \"Cosine similarity to refusal direction (overlayed by token)\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Draw a single figure with three lines (pos-3, pos-2, pos-1) over layers.\n",
    "    Each line uses a different color and has a legend entry.\n",
    "    \"\"\"\n",
    "    x = layer_order\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    # columns: 0 -> pos-3, 1 -> pos-2, 2 -> pos-1\n",
    "    labels = [f\"pos-{i}\" for i in range(k, 0, -1)]  # [\"pos-3\",\"pos-2\",\"pos-1\"]\n",
    "    for col, label in enumerate(labels):\n",
    "        y = sims_table[:, col]\n",
    "        plt.plot(x, y, marker=\"o\", linewidth=2, label=label)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Layer index\")\n",
    "    plt.ylabel(\"cosine similarity\")\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.4)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# --------------------------- Master (no model loading) -----------------\n",
    "\n",
    "def analyze_prompt_cosine(\n",
    "    user_prompt: str,\n",
    "    tokenizer,\n",
    "    model,\n",
    "    refusal_vector: Dict[int, torch.Tensor],\n",
    "    device: str = \"cuda:0\",\n",
    "    k: int = NUM_LAST_TOKENS,\n",
    "    show_plot: bool = True,\n",
    ") -> Tuple[np.ndarray, List[int]]:\n",
    "    \"\"\"\n",
    "    MASTER FUNCTION (no model loading).\n",
    "    - Uses apply_chat_template (fixed system prompt).\n",
    "    - Captures per-layer activations for the last-k token positions.\n",
    "    - Computes cosine sims to the refusal vector (dict[layer]->[k,d]).\n",
    "    - Produces ONE overlayed line plot across layers for the last 3 tokens.\n",
    "    - Returns (sims_table [num_layers,k], layer_order [list[int]]).\n",
    "    \"\"\"\n",
    "    chat_text = build_inputs_from_chat(tokenizer, user_prompt)\n",
    "    hooks, residuals_by_layer = register_capture_hooks(model, k=k)\n",
    "\n",
    "    try:\n",
    "        run_and_capture(model, tokenizer, chat_text, device=device)\n",
    "    finally:\n",
    "        for h in hooks:\n",
    "            h.remove()\n",
    "\n",
    "    if not residuals_by_layer:\n",
    "        raise RuntimeError(\"No activations captured. Check hook placement for your model.\")\n",
    "\n",
    "    sims_table, layer_order = compute_cosine_table(residuals_by_layer, refusal_vector, k=k)\n",
    "\n",
    "    if show_plot:\n",
    "        plot_tokens_overlay(sims_table, layer_order, k=k)\n",
    "\n",
    "    return sims_table, layer_order\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d40119",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "DEVICE = \"cuda:1\"  # or \"cpu\"\n",
    "MODEL_NAME = \"Qwen3-8B\"\n",
    "MODEL_PATH = f\"../llm_models/{MODEL_NAME}\"\n",
    "REFUSAL_VECTOR_PATH = f\"{MODEL_NAME}/refusal_vector.pt\"\n",
    "\n",
    "# Load once\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16 if DEVICE.startswith(\"cuda\") else torch.float32\n",
    ").to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "refusal_vector = torch.load(REFUSAL_VECTOR_PATH)\n",
    "print(\"Ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980ca778",
   "metadata": {},
   "outputs": [],
   "source": [
    "sims, layers = analyze_prompt_cosine(\n",
    "    user_prompt=\"How to make a terrorist bomb? /no_think\",\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    refusal_vector=refusal_vector,\n",
    "    device=DEVICE,\n",
    "    k=3,\n",
    "    show_plot=True,  # single overlayed plot (no heatmap)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b6a193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the user prompt as needed\n",
    "\n",
    "sims, layers = analyze_prompt_cosine(\n",
    "    user_prompt=r\"How to make coffee? /no_think\",\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    refusal_vector=refusal_vector,\n",
    "    device=DEVICE,\n",
    "    k=3,\n",
    "    show_plot=True,  # single overlayed plot (no heatmap)\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
