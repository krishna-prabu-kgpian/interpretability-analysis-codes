{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92781ac5",
   "metadata": {},
   "source": [
    "# Attention Patching Experiments\n",
    "\n",
    "This notebook demonstrates attention patching techniques to transfer refusal behaviors between different prompts. The experiment patches attention/MLP activations from a \"refusal\" prompt into a \"benign\" prompt to study how refusal patterns can be transferred.\n",
    "\n",
    "## How to Use\n",
    "1. **Run cells sequentially** - Each section builds on the previous\n",
    "2. **Modify prompts** - Change refusal_chat and benign_chat to test different scenarios\n",
    "3. **Adjust patching parameters** - Configure PATCH_LAYERS, PATCH_STEPS, etc.\n",
    "4. **Compare outputs** - Analyze how patching affects model responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18999c5",
   "metadata": {},
   "source": [
    "## 1. Import Libraries\n",
    "\n",
    "Import required libraries and disable gradients for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece0214c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2594fd80",
   "metadata": {},
   "source": [
    "## 2. Model Setup\n",
    "\n",
    "Load the model and tokenizer for patching experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e53ca70",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"../llm_models/Qwen3-0.6B\"\n",
    "DEVICE = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
    ").to(DEVICE).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79d62d9",
   "metadata": {},
   "source": [
    "## 3. Prompt Definitions\n",
    "\n",
    "Define the \"donor\" (refusal) and \"recipient\" (benign) prompts for patching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ab1196",
   "metadata": {},
   "outputs": [],
   "source": [
    "refusal_chat = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"How to make cocaine? /no_think\"}]\n",
    "benign_chat  = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"How make coffee? /no_think\"}]\n",
    "\n",
    "# benign_chat = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"How to make cocaine? /no_think\"}]\n",
    "# refusal_chat  = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"How to make coffee? /no_think\"}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ad161a",
   "metadata": {},
   "source": [
    "## 4. Patching Configuration\n",
    "\n",
    "Configure which layers and components to patch, and for how many generation steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92753fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "GEN_STEPS = 256             # number of semantic tokens to generate\n",
    "PATCH_LAYERS = [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22]       # [] = none, None = all\n",
    "PATCH_ATTN = True\n",
    "PATCH_MLP  = True\n",
    "PATCH_STEPS = 10\n",
    "STOP_ID = 151643   # custom stop token\n",
    "\n",
    "SCAFFOLD_IDS = [151667, 271, 151668, 271]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a79969",
   "metadata": {},
   "source": [
    "## 5. Phase A: Capture Donor Activations\n",
    "\n",
    "Generate from the refusal prompt while capturing attention and MLP activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54611497",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(chat):\n",
    "    return tok.apply_chat_template(chat, add_generation_prompt=True, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "def step_decode(m, ids, pkv):\n",
    "    out = m(input_ids=ids, past_key_values=pkv, use_cache=True)\n",
    "    logits = out.logits[:, -1, :]\n",
    "    next_id = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "    return next_id, out.past_key_values, out\n",
    "\n",
    "# ---------------- Phase A: Donor (refusal) ----------------\n",
    "A_ids = encode(refusal_chat); A_pkv = None\n",
    "for i in range(4):  # scaffold\n",
    "    tid, A_pkv, _ = step_decode(model, A_ids, A_pkv)\n",
    "    A_ids = tid\n",
    "\n",
    "donor_cache = {}\n",
    "def make_cap(name):\n",
    "    def hook(_, __, out):\n",
    "        out0 = out[0] if isinstance(out, tuple) else out\n",
    "        donor_cache[name] = out0[:, -1, :].detach().clone()\n",
    "    return hook\n",
    "\n",
    "layer_list = range(len(model.model.layers)) if PATCH_LAYERS is None else PATCH_LAYERS\n",
    "hooksA = []\n",
    "for i in layer_list:\n",
    "    if PATCH_ATTN:\n",
    "        hooksA.append(model.model.layers[i].self_attn.register_forward_hook(make_cap(f\"attn_{i}\")))\n",
    "    if PATCH_MLP:\n",
    "        hooksA.append(model.model.layers[i].mlp.register_forward_hook(make_cap(f\"mlp_{i}\")))\n",
    "\n",
    "donor_steps = []\n",
    "tokens_A = []\n",
    "for s in range(GEN_STEPS):\n",
    "    tid, A_pkv, _ = step_decode(model, A_ids, A_pkv)\n",
    "    next_token = int(tid)\n",
    "    donor_steps.append({k:v for k,v in donor_cache.items()})\n",
    "    tokens_A.append(next_token)\n",
    "    if next_token == STOP_ID:\n",
    "        break\n",
    "    A_ids = tid\n",
    "for h in hooksA: h.remove()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032cd0a6",
   "metadata": {},
   "source": [
    "## 6. Phase B: Generate Clean Baseline\n",
    "\n",
    "Generate from the benign prompt without any patching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63d3fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "B_ids = encode(benign_chat); B_pkv = None\n",
    "for i in range(4):\n",
    "    tid, B_pkv, _ = step_decode(model, B_ids, B_pkv)\n",
    "    B_ids = tid\n",
    "\n",
    "tokens_B = []\n",
    "for s in range(GEN_STEPS):\n",
    "    tid, B_pkv, _ = step_decode(model, B_ids, B_pkv)\n",
    "    next_token = int(tid)\n",
    "    tokens_B.append(next_token)\n",
    "    if next_token == STOP_ID:\n",
    "        break\n",
    "    B_ids = tid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf176fbd",
   "metadata": {},
   "source": [
    "## 7. Phase C: Apply Patches\n",
    "\n",
    "Generate from benign prompt while patching in the donor activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a4dd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "C_ids = encode(benign_chat); C_pkv = None\n",
    "for i in range(4):\n",
    "    tid, C_pkv, _ = step_decode(model, C_ids, C_pkv)\n",
    "    C_ids = tid\n",
    "\n",
    "step_idx = {'val':0}\n",
    "def make_patch(key):\n",
    "    def hook(_, __, out):\n",
    "        out0, *rest = out if isinstance(out, tuple) else (out,)\n",
    "        if step_idx['val'] < PATCH_STEPS:\n",
    "            donor = donor_steps[step_idx['val']].get(key, None)\n",
    "            if donor is not None:\n",
    "                out0 = out0.clone()\n",
    "                out0[:, -1, :] = donor\n",
    "        return (out0, *rest) if rest else out0\n",
    "    return hook\n",
    "\n",
    "hooksC = []\n",
    "for i in layer_list:\n",
    "    if PATCH_ATTN:\n",
    "        hooksC.append(model.model.layers[i].self_attn.register_forward_hook(make_patch(f\"attn_{i}\")))\n",
    "    if PATCH_MLP:\n",
    "        hooksC.append(model.model.layers[i].mlp.register_forward_hook(make_patch(f\"mlp_{i}\")))\n",
    "        \n",
    "tokens_C = []\n",
    "for s in range(GEN_STEPS):\n",
    "    tid, C_pkv, _ = step_decode(model, C_ids, C_pkv)\n",
    "    next_token = int(tid)\n",
    "    tokens_C.append(next_token)\n",
    "    if next_token == STOP_ID:\n",
    "        break\n",
    "    C_ids = tid\n",
    "    step_idx['val'] += 1\n",
    "for h in hooksC: h.remove()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e3eb7f",
   "metadata": {},
   "source": [
    "## 8. Results - Token-by-Token Comparison\n",
    "\n",
    "Compare generated tokens step by step across all three conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c97ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Side-by-side (A=Refusal, B=Clean, C=Patched B) ===\")\n",
    "maxlen = max(len(tokens_A), len(tokens_B), len(tokens_C))\n",
    "\n",
    "for s in range(maxlen):\n",
    "    tA = tok.decode([tokens_A[s]], skip_special_tokens=False) if s < len(tokens_A) else \"\"\n",
    "    tB = tok.decode([tokens_B[s]], skip_special_tokens=False) if s < len(tokens_B) else \"\"\n",
    "    tC = tok.decode([tokens_C[s]], skip_special_tokens=False) if s < len(tokens_C) else \"\"\n",
    "    print(f\"Step {s:02d}:  A={repr(tA):12s} | B={repr(tB):12s} | C={repr(tC):12s}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00781a4",
   "metadata": {},
   "source": [
    "## 9. Results - Full Text Output\n",
    "\n",
    "View the complete generated responses for all three conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cec7877",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"A==Refusal: \\n\\n\", tok.decode(tokens_A, skip_special_tokens=True), \"\\n\\n\")\n",
    "print(\"B==Normal: \\n\\n\", tok.decode(tokens_B, skip_special_tokens=True), \"\\n\\n\")    \n",
    "print(\"C==Patched: \\n\\n\", tok.decode(tokens_C, skip_special_tokens=True), \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4162ee0d",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrates the attention patching approach:\n",
    "\n",
    "✅ **Full Layer Patching**: Patches entire attention/MLP outputs across specified layers  \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
