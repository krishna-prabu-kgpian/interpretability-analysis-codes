{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92781ac5",
   "metadata": {},
   "source": [
    "# Attention Patching Experiments\n",
    "\n",
    "This notebook demonstrates attention patching techniques to transfer refusal behaviors between different prompts. The experiment patches attention/MLP activations from a \"refusal\" prompt into a \"benign\" prompt to study how refusal patterns can be transferred.\n",
    "\n",
    "## How to Use\n",
    "1. **Run cells sequentially** - Each section builds on the previous\n",
    "2. **Modify prompts** - Change refusal_chat and benign_chat to test different scenarios\n",
    "3. **Adjust patching parameters** - Configure PATCH_LAYERS, PATCH_STEPS, etc.\n",
    "4. **Compare outputs** - Analyze how patching affects model responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18999c5",
   "metadata": {},
   "source": [
    "## 1. Import Libraries\n",
    "\n",
    "Import required libraries and disable gradients for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece0214c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2594fd80",
   "metadata": {},
   "source": [
    "## 2. Model Setup\n",
    "\n",
    "Load the model and tokenizer for patching experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e53ca70",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"../llm_models/Qwen3-0.6B\"\n",
    "DEVICE = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
    ").to(DEVICE).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79d62d9",
   "metadata": {},
   "source": [
    "## 3. Prompt Definitions\n",
    "\n",
    "Define the \"donor\" (refusal) and \"recipient\" (benign) prompts for patching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ab1196",
   "metadata": {},
   "outputs": [],
   "source": [
    "refusal_chat = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"How to make cocaine? /no_think\"}]\n",
    "benign_chat  = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"How do plants do photosynthesis? /no_think\"}]\n",
    "\n",
    "# benign_chat = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"How to make cocaine? /no_think\"}]\n",
    "# refusal_chat  = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"How to make coffee? /no_think\"}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ad161a",
   "metadata": {},
   "source": [
    "## 4. Patching Configuration\n",
    "\n",
    "Configure which layers and components to patch, and for how many generation steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92753fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "GEN_STEPS = 256             # number of semantic tokens to generate\n",
    "PATCH_LAYERS = [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22]       # [] = none, None = all\n",
    "PATCH_ATTN = True\n",
    "PATCH_MLP  = True\n",
    "PATCH_STEPS = 10\n",
    "STOP_ID = 151643   # custom stop token\n",
    "\n",
    "SCAFFOLD_IDS = [151667, 271, 151668, 271]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a79969",
   "metadata": {},
   "source": [
    "## 5. Phase A: Capture Donor Activations\n",
    "\n",
    "Generate from the refusal prompt while capturing attention and MLP activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54611497",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(chat):\n",
    "    return tok.apply_chat_template(chat, add_generation_prompt=True, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "def step_decode(m, ids, pkv):\n",
    "    out = m(input_ids=ids, past_key_values=pkv, use_cache=True)\n",
    "    logits = out.logits[:, -1, :]\n",
    "    next_id = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "    return next_id, out.past_key_values, out\n",
    "\n",
    "# ---------------- Phase A: Donor (refusal) ----------------\n",
    "A_ids = encode(refusal_chat); A_pkv = None\n",
    "for i in range(4):  # scaffold\n",
    "    tid, A_pkv, _ = step_decode(model, A_ids, A_pkv)\n",
    "    A_ids = tid\n",
    "\n",
    "donor_cache = {}\n",
    "def make_cap(name):\n",
    "    def hook(_, __, out):\n",
    "        out0 = out[0] if isinstance(out, tuple) else out\n",
    "        donor_cache[name] = out0[:, -1, :].detach().clone()\n",
    "    return hook\n",
    "\n",
    "layer_list = range(len(model.model.layers)) if PATCH_LAYERS is None else PATCH_LAYERS\n",
    "hooksA = []\n",
    "for i in layer_list:\n",
    "    if PATCH_ATTN:\n",
    "        hooksA.append(model.model.layers[i].self_attn.register_forward_hook(make_cap(f\"attn_{i}\")))\n",
    "    if PATCH_MLP:\n",
    "        hooksA.append(model.model.layers[i].mlp.register_forward_hook(make_cap(f\"mlp_{i}\")))\n",
    "\n",
    "donor_steps = []\n",
    "tokens_A = []\n",
    "for s in range(GEN_STEPS):\n",
    "    tid, A_pkv, _ = step_decode(model, A_ids, A_pkv)\n",
    "    next_token = int(tid)\n",
    "    donor_steps.append({k:v for k,v in donor_cache.items()})\n",
    "    tokens_A.append(next_token)\n",
    "    if next_token == STOP_ID:\n",
    "        break\n",
    "    A_ids = tid\n",
    "for h in hooksA: h.remove()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032cd0a6",
   "metadata": {},
   "source": [
    "## 6. Phase B: Generate Clean Baseline\n",
    "\n",
    "Generate from the benign prompt without any patching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63d3fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "B_ids = encode(benign_chat); B_pkv = None\n",
    "for i in range(4):\n",
    "    tid, B_pkv, _ = step_decode(model, B_ids, B_pkv)\n",
    "    B_ids = tid\n",
    "\n",
    "tokens_B = []\n",
    "for s in range(GEN_STEPS):\n",
    "    tid, B_pkv, _ = step_decode(model, B_ids, B_pkv)\n",
    "    next_token = int(tid)\n",
    "    tokens_B.append(next_token)\n",
    "    if next_token == STOP_ID:\n",
    "        break\n",
    "    B_ids = tid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf176fbd",
   "metadata": {},
   "source": [
    "## 7. Phase C: Apply Patches\n",
    "\n",
    "Generate from benign prompt while patching in the donor activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a4dd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "C_ids = encode(benign_chat); C_pkv = None\n",
    "for i in range(4):\n",
    "    tid, C_pkv, _ = step_decode(model, C_ids, C_pkv)\n",
    "    C_ids = tid\n",
    "\n",
    "step_idx = {'val':0}\n",
    "def make_patch(key):\n",
    "    def hook(_, __, out):\n",
    "        out0, *rest = out if isinstance(out, tuple) else (out,)\n",
    "        if step_idx['val'] < PATCH_STEPS:\n",
    "            donor = donor_steps[step_idx['val']].get(key, None)\n",
    "            if donor is not None:\n",
    "                out0 = out0.clone()\n",
    "                out0[:, -1, :] = donor\n",
    "        return (out0, *rest) if rest else out0\n",
    "    return hook\n",
    "\n",
    "hooksC = []\n",
    "for i in layer_list:\n",
    "    if PATCH_ATTN:\n",
    "        hooksC.append(model.model.layers[i].self_attn.register_forward_hook(make_patch(f\"attn_{i}\")))\n",
    "    if PATCH_MLP:\n",
    "        hooksC.append(model.model.layers[i].mlp.register_forward_hook(make_patch(f\"mlp_{i}\")))\n",
    "        \n",
    "tokens_C = []\n",
    "for s in range(GEN_STEPS):\n",
    "    tid, C_pkv, _ = step_decode(model, C_ids, C_pkv)\n",
    "    next_token = int(tid)\n",
    "    tokens_C.append(next_token)\n",
    "    if next_token == STOP_ID:\n",
    "        break\n",
    "    C_ids = tid\n",
    "    step_idx['val'] += 1\n",
    "for h in hooksC: h.remove()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e3eb7f",
   "metadata": {},
   "source": [
    "## 8. Results - Token-by-Token Comparison\n",
    "\n",
    "Compare generated tokens step by step across all three conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c97ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Side-by-side (A=Refusal, B=Clean, C=Patched B) ===\")\n",
    "maxlen = max(len(tokens_A), len(tokens_B), len(tokens_C))\n",
    "\n",
    "for s in range(maxlen):\n",
    "    tA = tok.decode([tokens_A[s]], skip_special_tokens=False) if s < len(tokens_A) else \"\"\n",
    "    tB = tok.decode([tokens_B[s]], skip_special_tokens=False) if s < len(tokens_B) else \"\"\n",
    "    tC = tok.decode([tokens_C[s]], skip_special_tokens=False) if s < len(tokens_C) else \"\"\n",
    "    print(f\"Step {s:02d}:  A={repr(tA):12s} | B={repr(tB):12s} | C={repr(tC):12s}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00781a4",
   "metadata": {},
   "source": [
    "## 9. Results - Full Text Output\n",
    "\n",
    "View the complete generated responses for all three conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cec7877",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"A==Refusal: \\n\\n\", tok.decode(tokens_A, skip_special_tokens=True), \"\\n\\n\")\n",
    "print(\"B==Normal: \\n\\n\", tok.decode(tokens_B, skip_special_tokens=True), \"\\n\\n\")    \n",
    "print(\"C==Patched: \\n\\n\", tok.decode(tokens_C, skip_special_tokens=True), \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c6cbc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dc838392",
   "metadata": {},
   "source": [
    "## 10. Head-Specific Patching Configuration\n",
    "\n",
    "Configure which specific attention heads to patch in each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c804863",
   "metadata": {},
   "outputs": [],
   "source": [
    "GEN_STEPS   = 256\n",
    "PATCH_STEPS = 10\n",
    "STOP_ID     = 151643\n",
    "\n",
    "# Dict: None=all heads, []=no heads, list=those heads\n",
    "PATCH_HEADS = {15: [1, 5, 6, 10, 11, 13, 15], 16: [], 17: None}\n",
    "# PATCH_HEADS = {15: None, 16: None, 17: None}\n",
    "\n",
    "refusal_chat = [{\"role\":\"system\",\"content\":\"You are a helpful assistant.\"},\n",
    "                {\"role\":\"user\",\"content\":\"How to make cocaine? /no_think\"}]\n",
    "benign_chat  = [{\"role\":\"system\",\"content\":\"You are a helpful assistant.\"},\n",
    "                {\"role\":\"user\",\"content\":\"How do plants do photosynthesis? /no_think\"}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9762ba2c",
   "metadata": {},
   "source": [
    "## 11. Helper Functions\n",
    "\n",
    "Define encoding and decoding utilities for head-specific patching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508074bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(chat):\n",
    "    return tok.apply_chat_template(chat, add_generation_prompt=True, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "def step_decode(m, ids, pkv):\n",
    "    out = m(input_ids=ids, past_key_values=pkv, use_cache=True)\n",
    "    logits = out.logits[:, -1, :]\n",
    "    next_id = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "    return next_id, out.past_key_values, out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd54e7b8",
   "metadata": {},
   "source": [
    "## 12. Capture Head-Specific Activations\n",
    "\n",
    "Capture attention outputs shaped as [batch, heads, head_dim] for targeted patching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3240dd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "A_ids = encode(refusal_chat); A_pkv = None\n",
    "for _ in range(4): tid, A_pkv, _ = step_decode(model, A_ids, A_pkv); A_ids = tid\n",
    "\n",
    "donor_cache = {}\n",
    "def make_cap(layer, heads):\n",
    "    def hook(_, __, out):\n",
    "        out0 = out[0] if isinstance(out, tuple) else out\n",
    "        B, T, D = out0.shape\n",
    "        H = heads if heads is not None else model.config.num_attention_heads\n",
    "        hd = D // model.config.num_attention_heads\n",
    "        shaped = out0.view(B, T, model.config.num_attention_heads, hd)\n",
    "        donor_cache[layer] = shaped[:, -1, :, :].detach().clone()  # [B, H, hd]\n",
    "    return hook\n",
    "\n",
    "hooksA = []\n",
    "for layer, heads in PATCH_HEADS.items():\n",
    "    if heads != []:  # capture unless explicitly empty\n",
    "        hooksA.append(model.model.layers[layer].self_attn.register_forward_hook(make_cap(layer, heads)))\n",
    "\n",
    "donor_steps, tokens_A = [], []\n",
    "for s in range(GEN_STEPS):\n",
    "    tid, A_pkv, _ = step_decode(model, A_ids, A_pkv)\n",
    "    donor_steps.append({k:v for k,v in donor_cache.items()})\n",
    "    tokens_A.append(int(tid))\n",
    "    if tokens_A[-1] == STOP_ID: break\n",
    "    A_ids = tid\n",
    "for h in hooksA: h.remove()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3a7290",
   "metadata": {},
   "source": [
    "## 13. Clean Baseline Generation\n",
    "\n",
    "Generate clean responses without patching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25fb37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "B_ids = encode(benign_chat); B_pkv = None\n",
    "for _ in range(4): tid, B_pkv, _ = step_decode(model, B_ids, B_pkv); B_ids = tid\n",
    "tokens_B = []\n",
    "for s in range(GEN_STEPS):\n",
    "    tid, B_pkv, _ = step_decode(model, B_ids, B_pkv)\n",
    "    tokens_B.append(int(tid))\n",
    "    if tokens_B[-1] == STOP_ID: break\n",
    "    B_ids = tid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abab7e9c",
   "metadata": {},
   "source": [
    "## 14. Apply Head-Specific Patches\n",
    "\n",
    "Patch only specific attention heads while leaving others unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db4cca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "C_ids = encode(benign_chat); C_pkv = None\n",
    "for _ in range(4): tid, C_pkv, _ = step_decode(model, C_ids, C_pkv); C_ids = tid\n",
    "\n",
    "step_idx = {'val':0}\n",
    "def make_patch(layer, heads):\n",
    "    def hook(_, __, out):\n",
    "        out0, *rest = out if isinstance(out, tuple) else (out,)\n",
    "        if step_idx['val'] < PATCH_STEPS:\n",
    "            donor = donor_steps[step_idx['val']].get(layer, None)\n",
    "            if donor is not None:\n",
    "                B, T, D = out0.shape\n",
    "                hd = D // model.config.num_attention_heads\n",
    "                out_heads = out0.view(B, T, model.config.num_attention_heads, hd).clone()\n",
    "                # Decide which heads to patch\n",
    "                target_heads = range(model.config.num_attention_heads) if heads is None else heads\n",
    "                for h in target_heads:\n",
    "                    out_heads[:, -1, h, :] = donor[:, h, :]\n",
    "                out0 = out_heads.view(B, T, D)\n",
    "        return (out0, *rest) if rest else out0\n",
    "    return hook\n",
    "\n",
    "hooksC = []\n",
    "for layer, heads in PATCH_HEADS.items():\n",
    "    if heads != []:  # patch unless explicitly empty\n",
    "        hooksC.append(model.model.layers[layer].self_attn.register_forward_hook(make_patch(layer, heads)))\n",
    "\n",
    "tokens_C = []\n",
    "for s in range(GEN_STEPS):\n",
    "    tid, C_pkv, _ = step_decode(model, C_ids, C_pkv)\n",
    "    tokens_C.append(int(tid))\n",
    "    if tokens_C[-1] == STOP_ID: break\n",
    "    C_ids = tid\n",
    "    step_idx['val'] += 1\n",
    "for h in hooksC: h.remove()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98168027",
   "metadata": {},
   "source": [
    "## 15. Head Patching Results - Token Comparison\n",
    "\n",
    "Compare outputs from head-specific patching experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3742bad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Side-by-side (A=Refusal, B=Clean, C=Patched B) ===\")\n",
    "maxlen = max(len(tokens_A), len(tokens_B), len(tokens_C))\n",
    "for s in range(maxlen):\n",
    "    tA = tok.decode([tokens_A[s]], skip_special_tokens=False) if s < len(tokens_A) else \"\"\n",
    "    tB = tok.decode([tokens_B[s]], skip_special_tokens=False) if s < len(tokens_B) else \"\"\n",
    "    tC = tok.decode([tokens_C[s]], skip_special_tokens=False) if s < len(tokens_C) else \"\"\n",
    "    print(f\"Step {s:02d}:  A={repr(tA):12s} | B={repr(tB):12s} | C={repr(tC):12s}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546eeb08",
   "metadata": {},
   "source": [
    "## 16. Head Patching Results - Full Text\n",
    "\n",
    "View complete responses from the head-specific patching experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff11bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"A==Refusal: \\n\\n\", tok.decode(tokens_A, skip_special_tokens=True), \"\\n\\n\")\n",
    "print(\"B==Normal: \\n\\n\", tok.decode(tokens_B, skip_special_tokens=True), \"\\n\\n\")\n",
    "print(\"C==Patched: \\n\\n\", tok.decode(tokens_C, skip_special_tokens=True), \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4162ee0d",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrates two attention patching approaches:\n",
    "\n",
    "✅ **Full Layer Patching**: Patches entire attention/MLP outputs across specified layers  \n",
    "✅ **Head-Specific Patching**: Targets individual attention heads for more precise control\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73f3dd8",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
