{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "742faa0c",
   "metadata": {},
   "source": [
    "# Identifying Refusal Location for Harmful vs Harmless Prompt Analysis\n",
    "\n",
    "This notebook extracts and analyzes internal activations from language models when processing harmful vs harmless prompts. It creates \"refusal vectors\" that can be used to understand how models internally distinguish between safe and unsafe content.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The notebook performs the following main steps:\n",
    "1. **Data Extraction**: Extract activations from model layers for harmful and harmless prompts\n",
    "2. **Mean Computation**: Calculate mean activations for each category\n",
    "3. **Refusal Vector Creation**: Compute difference vectors between harmful and harmless means\n",
    "4. **Visualization**: Plot cosine similarities to analyze differences across layers\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- CUDA-capable GPU (configured for `cuda:1`)\n",
    "- Required datasets stored locally in `../dataset_folder/`\n",
    "- Pre-trained model stored in `../llm_models/`\n",
    "- Sufficient disk space for activation storage (~GBs depending on dataset size)\n",
    "\n",
    "## Usage Instructions\n",
    "\n",
    "1. **Configure Model and Datasets**: Modify the configuration section below\n",
    "2. **Set Processing Mode**: Choose \"harmful\" or \"harmless\" for data extraction\n",
    "3. **Run Extraction**: Execute the activation extraction pipeline\n",
    "4. **Compute Statistics**: Calculate means and refusal vectors\n",
    "5. **Analyze Results**: View visualizations and save outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a501f49",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Setup\n",
    "\n",
    "Import required libraries for model loading, data processing, and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e4a7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TextStreamer\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = \"cuda:1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3014fd82",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Configure dataset sources, model paths, and processing parameters.\n",
    "\n",
    "### Key Parameters:\n",
    "- **DATASET_SOURCE_HARMFUL**: PKU-SafeRLHF dataset for harmful prompts\n",
    "- **DATASET_SOURCE_HARMLESS**: Alpaca dataset for harmless instructions  \n",
    "- **TARGET_SIZE**: Number of examples to process (20,000)\n",
    "- **MODEL_NAME**: Model identifier for file naming\n",
    "- **BATCH_SIZE**: Processing batch size for memory management\n",
    "- **NUM_LAST_TOKENS**: Number of final tokens to analyze (3)\n",
    "\n",
    "‚ö†Ô∏è **Important**: Ensure the model path `../llm_models/{MODEL_NAME}` exists and datasets are available in `../dataset_folder/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7784a704",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_SOURCE_HARMFUL = \"PKU-Alignment/PKU-SafeRLHF-prompt\"\n",
    "DATASET_SOURCE_HARMLESS = \"tatsu-lab/alpaca\"\n",
    "\n",
    "PROMPT_COLUMN_HARMFUL = \"prompt\"\n",
    "PROMPT_COLUMN_HARMLESS = \"instruction\"\n",
    "\n",
    "TARGET_SIZE = 20000\n",
    "\n",
    "MODEL_NAME = \"Qwen3-0.6B\"\n",
    "MODEL_PATH = f\"../llm_models/{MODEL_NAME}\"\n",
    "\n",
    "OUTPUT_DIR_HARMFUL = f\"{MODEL_NAME}/harmful_prompt_activations\"\n",
    "OUTPUT_DIR_HARMLESS = f\"{MODEL_NAME}/harmless_prompt_activations\"\n",
    "os.makedirs(OUTPUT_DIR_HARMFUL, exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR_HARMLESS, exist_ok=True)\n",
    "\n",
    "BATCH_SIZE = 512\n",
    "NUM_LAST_TOKENS = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e7a326",
   "metadata": {},
   "source": [
    "## 3. Load Model and Tokenizer\n",
    "\n",
    "Load the language model and tokenizer. The model is loaded in half precision (float16) to save memory and moved to the specified GPU device.\n",
    "\n",
    "üîß **Note**: This may take several minutes depending on model size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d962cbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16\n",
    ").to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f208fc8",
   "metadata": {},
   "source": [
    "## 4. Set Processing Mode\n",
    "\n",
    "Choose which type of data to process. You need to run the next few cells **twice**:\n",
    "1. First with `MODE = \"harmful\"` to extract harmful prompt activations\n",
    "2. Then with `MODE = \"harmless\"` to extract harmless prompt activations\n",
    "\n",
    "üí° **Tip**: **ONLY** after completing both runs, proceed to the analysis sections at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e4a916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODE = \"harmful\"\n",
    "MODE = \"harmless\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1259ad",
   "metadata": {},
   "source": [
    "## 5. Load and Sample Dataset\n",
    "\n",
    "Load the appropriate dataset based on the selected mode and sample examples to reach the target size. \n",
    "\n",
    "The sampling uses a step size to evenly distribute examples across the full dataset rather than taking only the first N examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0675fd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODE == \"harmful\":\n",
    "    dataset = load_from_disk(f\"../dataset_folder/{DATASET_SOURCE_HARMFUL}\")\n",
    "    if \"train\" in dataset:\n",
    "        ds = dataset[\"train\"]\n",
    "    else:\n",
    "        ds = dataset\n",
    "    all_prompts = ds[PROMPT_COLUMN_HARMFUL]\n",
    "    total_examples = len(all_prompts)\n",
    "\n",
    "    step = total_examples // TARGET_SIZE\n",
    "    all_prompts = all_prompts[::step][:TARGET_SIZE]\n",
    "else:\n",
    "    dataset = load_from_disk(f\"../dataset_folder/{DATASET_SOURCE_HARMLESS}\")\n",
    "    if \"train\" in dataset:\n",
    "        ds = dataset[\"train\"]\n",
    "    else:\n",
    "        ds = dataset\n",
    "    all_prompts = ds[PROMPT_COLUMN_HARMLESS]\n",
    "    total_examples = len(all_prompts)\n",
    "\n",
    "    step = total_examples // TARGET_SIZE\n",
    "    all_prompts = all_prompts[::step][:TARGET_SIZE]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6432519f",
   "metadata": {},
   "source": [
    "## 6. Setup Activation Hooks\n",
    "\n",
    "Create forward hooks to capture internal activations from each transformer layer. The hooks extract the last N tokens' activations (set before), which are most relevant for understanding the model's final decision-making process.\n",
    "\n",
    "üß† **Technical Detail**: Forward hooks intercept the output of each layer during the forward pass without affecting the model's computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc8bb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals_by_layer = {}\n",
    "\n",
    "def make_hook(layer_idx):\n",
    "    \"\"\"\n",
    "    Returns a forward hook function that, when attached to a transformer layer,\n",
    "    captures the activations (output) of that layer for the last NUM_LAST_TOKENS tokens.\n",
    "    \"\"\"\n",
    "    def hook_fn(module, inputs, outputs):\n",
    "        with torch.no_grad():\n",
    "            activation = outputs[0] if isinstance(outputs, tuple) else outputs\n",
    "            act_cpu = activation.detach().cpu()\n",
    "            seq_len = act_cpu.size(1)\n",
    "\n",
    "            if seq_len >= NUM_LAST_TOKENS:\n",
    "                residuals_by_layer[layer_idx] = act_cpu[0, -NUM_LAST_TOKENS :, :].clone()\n",
    "            else:\n",
    "                residuals_by_layer[layer_idx] = act_cpu[0, :, :].clone()\n",
    "\n",
    "    return hook_fn\n",
    "\n",
    "hooks = []\n",
    "for idx, layer in enumerate(model.model.layers):\n",
    "    hooks.append(layer.register_forward_hook(make_hook(idx)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d09332",
   "metadata": {},
   "source": [
    "## 7. Extract Activations\n",
    "\n",
    "Process each prompt through the model and collect activations from all layers. The data is saved in batches to manage memory usage.\n",
    "\n",
    "‚è±Ô∏è **Processing Time**: This step may take 80-140 minutes depending on sample size chosen and gpu hardware available.\n",
    "\n",
    "### Process:\n",
    "1. Format each prompt using the model's chat template\n",
    "2. Run forward pass to trigger hooks and capture activations  \n",
    "3. Save activations in batches to disk\n",
    "4. Clean up hooks when complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9934ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_data = []\n",
    "batch_counter = 0\n",
    "\n",
    "for idx, prompt in enumerate(all_prompts):\n",
    "    residuals_by_layer.clear()\n",
    "    chat = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\",   \"content\": prompt}\n",
    "    ]\n",
    "    chat_text = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        chat_text,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        _ = model(**inputs)\n",
    "        \n",
    "    example_record = {\n",
    "        \"prompt\": prompt,\n",
    "        \"residuals\": {layer_idx: tensor for layer_idx, tensor in residuals_by_layer.items()}\n",
    "    }\n",
    "    batch_data.append(example_record)\n",
    "\n",
    "    is_last_example = (idx + 1) == len(all_prompts)\n",
    "    if (len(batch_data) >= BATCH_SIZE) or is_last_example:\n",
    "        batch_id = batch_counter\n",
    "        if MODE == \"harmful\":\n",
    "            save_path = os.path.join(OUTPUT_DIR_HARMFUL, f\"batch_{batch_id:04d}.pt\")\n",
    "        else:\n",
    "            save_path = os.path.join(OUTPUT_DIR_HARMLESS, f\"batch_{batch_id:04d}.pt\")\n",
    "        torch.save(batch_data, save_path)\n",
    "        print(f\"Saved batch {batch_id} with {len(batch_data)} examples ‚Üí {save_path}\")\n",
    "        batch_counter += 1\n",
    "        batch_data = [] \n",
    "\n",
    "for h in hooks:\n",
    "    h.remove()\n",
    "\n",
    "print(\"‚úÖ Finished processing all examples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1410e16a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Statistical Analysis Section\n",
    "\n",
    "‚ö†Ô∏è **Prerequisites**: Run sections 4-7 **twice** (once for \"harmful\" and once for \"harmless\" mode) before proceeding.\n",
    "\n",
    "The following sections compute statistics and create refusal vectors from the extracted activations.\n",
    "\n",
    "### Setup Output Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e148b4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "HARMFUL_DIR = OUTPUT_DIR_HARMFUL\n",
    "HARMLESS_DIR = OUTPUT_DIR_HARMLESS\n",
    "\n",
    "OUTPUT_MEAN_HARMFUL   = f\"{MODEL_NAME}/harmful_mean.pt\"\n",
    "OUTPUT_MEAN_HARMLESS  = f\"{MODEL_NAME}/harmless_mean.pt\"\n",
    "OUTPUT_REFUSAL_VECTOR = f\"{MODEL_NAME}/refusal_vector.pt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20231f1f",
   "metadata": {},
   "source": [
    "### Define Mean Computation Function\n",
    "\n",
    "This function computes the mean activation across all examples for each layer and token position. It discards previously loaded data, to avoid loading all data into memory simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75cfb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_means_for_directory(dir_path):\n",
    "    \"\"\"\n",
    "    Iterates over all .pt files in `dir_path`, each of which is a list of dicts:\n",
    "        { \"prompt\": str,\n",
    "          \"residuals\": { layer_idx: Tensor[NUM_LAST_TOKENS, hidden_dim], ... }\n",
    "        }\n",
    "    Returns a dict: { layer_idx: Tensor[NUM_LAST_TOKENS, hidden_dim] } containing the float32 mean\n",
    "    across all examples in that directory.\n",
    "    \"\"\"\n",
    "    means = {}        \n",
    "    count = 0         \n",
    "\n",
    "    \n",
    "    filenames = sorted([f for f in os.listdir(dir_path) if f.endswith(\".pt\")])\n",
    "    for fname in filenames:\n",
    "        batch_path = os.path.join(dir_path, fname)\n",
    "        batch_list = torch.load(batch_path) \n",
    "\n",
    "        for example in batch_list:\n",
    "            count += 1\n",
    "            residuals = example[\"residuals\"] \n",
    "\n",
    "            if count == 1:\n",
    "                for layer_idx, tensor in residuals.items():\n",
    "                    means[layer_idx] = tensor.float().clone()\n",
    "            else:\n",
    "                for layer_idx, tensor in residuals.items():\n",
    "                    x = tensor.float()  \n",
    "                    old_mean = means[layer_idx]\n",
    "                    means[layer_idx] = old_mean + (x - old_mean) / count\n",
    "\n",
    "        del batch_list\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bae048",
   "metadata": {},
   "source": [
    "## 9. Compute Mean Activations\n",
    "\n",
    "Calculate the mean activation vectors for harmful and harmless prompts across all layers and token positions. These means represent the \"typical\" internal state when processing each type of content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a455b9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "means_harmful = compute_means_for_directory(HARMFUL_DIR)\n",
    "torch.save(means_harmful, OUTPUT_MEAN_HARMFUL)\n",
    "print(f\"Saved harmful means ‚Üí {OUTPUT_MEAN_HARMFUL}\")\n",
    "\n",
    "means_harmless = compute_means_for_directory(HARMLESS_DIR)\n",
    "torch.save(means_harmless, OUTPUT_MEAN_HARMLESS)\n",
    "print(f\"Saved harmless means ‚Üí {OUTPUT_MEAN_HARMLESS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad42299",
   "metadata": {},
   "source": [
    "## 10. Create Refusal Vector\n",
    "\n",
    "Compute the refusal vector by taking the difference between harmful and harmless mean activations. This vector represents the direction in activation space that distinguishes harmful from harmless content.\n",
    "\n",
    "**Formula**: `refusal_vector = mean_harmful - mean_harmless`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737b495c",
   "metadata": {},
   "outputs": [],
   "source": [
    "means_harmful = torch.load(OUTPUT_MEAN_HARMFUL)\n",
    "means_harmless = torch.load(OUTPUT_MEAN_HARMLESS)\n",
    "\n",
    "refusal_vector = {}\n",
    "for layer_idx in means_harmful:\n",
    "    refusal_vector[layer_idx] = means_harmful[layer_idx] - means_harmless[layer_idx]\n",
    "\n",
    "torch.save(refusal_vector, OUTPUT_REFUSAL_VECTOR)\n",
    "print(f\"Saved refusal vector ‚Üí {OUTPUT_REFUSAL_VECTOR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d064dbd",
   "metadata": {},
   "source": [
    "## 11. Visualization: Cosine Similarity Analysis\n",
    "\n",
    "Analyze how similar the harmful and harmless activations are across different layers and token positions. Lower cosine similarity indicates stronger differentiation between harmful and harmless content.\n",
    "\n",
    "### Interpretation:\n",
    "- **High similarity**: Layer shows little difference between harmful/harmless\n",
    "- **Comparitively Lower similarity**: Layer differentiates content types. In practise, there is hardly a dip of over 0.1.\n",
    "- **Patterns across tokens**: Shows which token positions are most informative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58b6306",
   "metadata": {},
   "outputs": [],
   "source": [
    "means_harmful = torch.load(OUTPUT_MEAN_HARMFUL)\n",
    "means_harmless = torch.load(OUTPUT_MEAN_HARMLESS)\n",
    "\n",
    "layer_indices = sorted(means_harmful.keys())\n",
    "cos_sims = {i: [] for i in range(NUM_LAST_TOKENS)}\n",
    "\n",
    "for layer_idx in layer_indices:\n",
    "    v_h = means_harmful[layer_idx]\n",
    "    v_safe = means_harmless[layer_idx]\n",
    "\n",
    "    for token_pos in range(NUM_LAST_TOKENS):\n",
    "        a = v_h[token_pos].unsqueeze(0)    \n",
    "        b = v_safe[token_pos].unsqueeze(0) \n",
    "        cos_val = F.cosine_similarity(a, b, dim=1).item()\n",
    "        cos_sims[token_pos].append(cos_val)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "for token_pos in range(NUM_LAST_TOKENS):\n",
    "    label = {\n",
    "        0: \"third‚Äêlast token\",\n",
    "        1: \"second‚Äêlast token\",\n",
    "        2: \"last token\"\n",
    "    }[token_pos]\n",
    "    plt.plot(layer_indices, cos_sims[token_pos], label=label)\n",
    "\n",
    "plt.xlabel(\"Layer Index\")\n",
    "plt.ylabel(\"Cosine Similarity\")\n",
    "plt.title(\"Cosine Similarity Between Harmful & Harmless Mean Vectors\\n(for Each of the Last 3 Token Positions)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8f0684",
   "metadata": {},
   "source": [
    "## 12. Refusal Vector Injection Testing\n",
    "\n",
    "Test the effectiveness of the created refusal vectors by using them in injection hooks during model generation. This allows you to see how the vectors affect model behavior when added or removed from activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d047f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import Dict, Iterable, List\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# ---------------------------\n",
    "# Hook utils\n",
    "# ---------------------------\n",
    "\n",
    "def _get_block_for_layer(model, layer_idx: int):\n",
    "    \"\"\"\n",
    "    Locate the transformer block at layer_idx for common HF CausalLMs.\n",
    "    Supports Qwen/LLaMA/Mistral-style (.model.layers) and GPT-2-style (.transformer.h).\n",
    "    \"\"\"\n",
    "    if hasattr(model, \"model\") and hasattr(model.model, \"layers\"):\n",
    "        layers = model.model.layers\n",
    "    elif hasattr(model, \"transformer\") and hasattr(model.transformer, \"h\"):\n",
    "        layers = model.transformer.h\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported model structure: cannot locate transformer blocks.\")\n",
    "\n",
    "    if not (0 <= layer_idx < len(layers)):\n",
    "        raise ValueError(f\"layer_idx {layer_idx} out of range [0, {len(layers)-1}]\")\n",
    "\n",
    "    return layers[layer_idx]\n",
    "\n",
    "\n",
    "def register_multi_refusal_edit(\n",
    "    model,\n",
    "    refusal_vector: Dict[int, torch.Tensor],   # layer_idx -> [NUM_LAST_TOKENS, hidden_dim]\n",
    "    plan: Dict[int, Iterable[int]],            # e.g., {10: [0,1], 11: [1]}\n",
    "    *,\n",
    "    num_last_tokens: int = 3,\n",
    "    mode: str = \"remove\",                      # \"remove\" -> project-out; \"add\" -> inject\n",
    "    add_strength: float = 1.0,                 # scale for \"add\" mode\n",
    ") -> List[torch.utils.hooks.RemovableHandle]:\n",
    "    \"\"\"\n",
    "    Register forward hooks to edit residuals at multiple (layer, offset) pairs.\n",
    "\n",
    "    Token offsets are from the END of the input sequence:\n",
    "      0 = last token, 1 = second-last, 2 = third-last (for num_last_tokens=3).\n",
    "\n",
    "    Modes:\n",
    "      - \"remove\": v <- v - proj_ref(v)    (project-out refusal direction)\n",
    "      - \"add\"   : v <- v + add_strength * ref_unit\n",
    "\n",
    "    Returns:\n",
    "      List of hook handles; call .remove() on each to undo.\n",
    "    \"\"\"\n",
    "    if mode not in {\"remove\", \"add\"}:\n",
    "        raise ValueError(\"mode must be either 'remove' or 'add'\")\n",
    "\n",
    "    handles: List[torch.utils.hooks.RemovableHandle] = []\n",
    "\n",
    "    # Prepack per-layer vectors keyed by offset\n",
    "    prepacked: Dict[int, Dict[int, torch.Tensor]] = {}\n",
    "    for layer_idx, offsets in plan.items():\n",
    "        if layer_idx not in refusal_vector:\n",
    "            raise KeyError(f\"refusal_vector has no entry for layer {layer_idx}\")\n",
    "        layer_t = refusal_vector[layer_idx]  # [num_last_tokens, hidden_dim]\n",
    "        if layer_t.size(0) != num_last_tokens:\n",
    "            raise ValueError(\n",
    "                f\"refusal_vector[{layer_idx}] has {layer_t.size(0)} rows, expected {num_last_tokens}\"\n",
    "            )\n",
    "\n",
    "        uniq = sorted(set(int(o) for o in offsets))\n",
    "        for o in uniq:\n",
    "            if not (0 <= o < num_last_tokens):\n",
    "                raise ValueError(f\"Offset {o} invalid for num_last_tokens={num_last_tokens}.\")\n",
    "\n",
    "        # Map offset_from_last -> stored index in your saved tensor\n",
    "        # Your storage: index 0 = third-last, 1 = second-last, 2 = last\n",
    "        # So stored_idx = num_last_tokens - 1 - offset\n",
    "        layer_vecs = {}\n",
    "        for o in uniq:\n",
    "            stored_idx = num_last_tokens - 1 - o\n",
    "            layer_vecs[o] = layer_t[stored_idx]   # [hidden_dim], float32 CPU\n",
    "        prepacked[layer_idx] = layer_vecs\n",
    "\n",
    "    # One hook per layer\n",
    "    for layer_idx, offset_to_vec in prepacked.items():\n",
    "        block = _get_block_for_layer(model, layer_idx)\n",
    "        offsets_sorted = sorted(offset_to_vec.keys())\n",
    "\n",
    "        def make_hook(offsets_sorted=offsets_sorted, offset_to_vec=offset_to_vec):\n",
    "            def hook_fn(module, inputs, output):\n",
    "                # Some models return a tuple; first element is hidden states\n",
    "                is_tuple = isinstance(output, tuple)\n",
    "                x = output[0] if is_tuple else output  # [B, T, H]\n",
    "                if x.dim() != 3 or x.size(1) == 0:\n",
    "                    return output\n",
    "\n",
    "                x_mod = x.clone()\n",
    "                B, T, H = x_mod.shape\n",
    "\n",
    "                for o in offsets_sorted:\n",
    "                    pos = T - 1 - o  # offset-from-end within current sequence\n",
    "                    if pos < 0:\n",
    "                        continue  # sequence shorter than requested offset; skip\n",
    "\n",
    "                    # Cast to current device/dtype and unit-normalize\n",
    "                    ref = offset_to_vec[o].to(device=x_mod.device, dtype=x_mod.dtype)\n",
    "                    denom = ref.norm()\n",
    "                    if torch.isnan(denom) or float(denom) < 1e-12:\n",
    "                        continue\n",
    "                    ref_unit = ref / denom\n",
    "\n",
    "                    if mode == \"remove\":\n",
    "                        v = x_mod[:, pos, :]                           # [B, H]\n",
    "                        proj = (v @ ref_unit)[:, None] * ref_unit[None, :]  # [B, H]\n",
    "                        x_mod[:, pos, :] = v - proj\n",
    "                    else:  # mode == \"add\"\n",
    "                        x_mod[:, pos, :] = x_mod[:, pos, :] + add_strength * ref_unit\n",
    "\n",
    "                return (x_mod,) + output[1:] if is_tuple else x_mod\n",
    "            return hook_fn\n",
    "\n",
    "        handle = block.register_forward_hook(make_hook())\n",
    "        handles.append(handle)\n",
    "\n",
    "    return handles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b643c43",
   "metadata": {},
   "source": [
    "### Setup Refusal Vector Parameters\n",
    "\n",
    "Configure which layer and token position to use for testing the injection hook. The refusal vector is loaded from the previously created file and prepared for injection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cbf810",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH, trust_remote_code=True, torch_dtype=torch.float16\n",
    ").to(device).eval()\n",
    "\n",
    "# Refusal vector\n",
    "refusal_vector = torch.load(r\"Qwen3-4B/refusal_vector.pt\")  # dict[layer_idx] -> [3, hidden_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30a0c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose plan: e.g. layer 10 remove offsets {0,1}; layer 11 add offset {1}\n",
    "plan_remove = {}\n",
    "plan_add    = {20: [0]}\n",
    "\n",
    "# Option A: REMOVE (project-out) at specified layers/offsets\n",
    "handles_remove = register_multi_refusal_edit(\n",
    "    model,\n",
    "    refusal_vector=refusal_vector,\n",
    "    plan=plan_remove,\n",
    "    num_last_tokens=3,\n",
    "    mode=\"remove\",\n",
    ")\n",
    "\n",
    "# Option B: ADD (inject) at specified layers/offsets (strength can be tuned)\n",
    "handles_add = register_multi_refusal_edit(\n",
    "    model,\n",
    "    refusal_vector=refusal_vector,\n",
    "    plan=plan_add,\n",
    "    num_last_tokens=3,\n",
    "    mode=\"add\",\n",
    "    add_strength=40,   # try values like 0.5, 1.0, 2.0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7136d1b4",
   "metadata": {},
   "source": [
    "### Test Injection with Sample Prompt\n",
    "\n",
    "Run a test using a potentially harmful prompt to see how the refusal vector affects model behavior. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b7d567",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"How to make coffee? /no_think\"}\n",
    "]\n",
    "chat_text = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "inputs = tokenizer(chat_text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out_ids = model.generate(**inputs, max_new_tokens=256, do_sample=False)\n",
    "print(tokenizer.decode(out_ids[0], skip_special_tokens=True))\n",
    "\n",
    "# Clean up hooks\n",
    "for h in handles_remove + handles_add:\n",
    "    h.remove()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c423a9eb",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook has successfully created a complete pipeline for refusal vector analysis and testing:\n",
    "\n",
    "### üìä **Analysis Components Created:**\n",
    "\n",
    "‚úÖ **Extracted Activations**: Saved in `{MODEL_NAME}/harmful_prompt_activations/` and `{MODEL_NAME}/harmless_prompt_activations/`\n",
    "\n",
    "‚úÖ **Mean Vectors**: Saved as `{MODEL_NAME}/harmful_mean.pt` and `{MODEL_NAME}/harmless_mean.pt`\n",
    "\n",
    "‚úÖ **Refusal Vector**: Saved as `{MODEL_NAME}/refusal_vector.pt`\n",
    "\n",
    "‚úÖ **Cosine Similarity Analysis**: Visualization showing layer-wise differences between harmful and harmless activations\n",
    "\n",
    "‚úÖ **Injection Hook System**: Precise vector injection/removal for model interventions\n",
    "\n",
    "### üìÅ **File Outputs:**\n",
    "All outputs are saved with the model name prefix for easy organization and comparison across different models. The refusal vectors can be directly used in other experiments and intervention studies."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deakin (3.12.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
